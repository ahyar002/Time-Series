# -*- coding: utf-8 -*-
"""Submission 2 - Time series covi-19 Indonesia

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TnN8mQf8YDFIseGERlC7fRVW3G8cUri0

Ahyar


 Bergabung sejak 03 Jun 2020

 Kota Bandung, Jawa Barat
"""

# import library
import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/covid_19_indonesia_time_series_all.csv')
df.head()

#  mengecek apakah ada nilai yang hilang dari dataset
df.isnull().sum()

# drop item yang tidak perlu
df.drop(["Location ISO Code", "New Cases", "New Deaths", "New Recovered", "New Active Cases", "Province", "Country", "Continent", "Island", "Time Zone", 
                "Special Status", "Total Regencies", "Total Cities", "Total Districts", "Total Urban Villages", "Total Rural Villages", "Area (km2)", 
                "New Cases per Million", "Total Cases per Million","New Deaths per Million", "Total Deaths per Million", "Case Fatality Rate",
                "Case Recovered Rate", "Growth Factor of New Cases","Growth Factor of New Deaths", "City or Regency"],axis=1,inplace=True)
df.head()

# menjadikan data array dan plot
dates = df['Date'].values
death  = df['Total Deaths'].values.astype('float32')

 
plt.figure(figsize=(20,5))
plt.plot(dates, death)
plt.title('Jumlah Kematian',
          fontsize=20);
plt.ylabel('Total Deaths')
plt.xlabel('Datetime')

# membagi data  Validation set sebesar 20% dari total dataset.
x_train, x_valid, y_train, y_valid = train_test_split(death, dates, train_size=0.8, test_size = 0.2, shuffle = False )

print('Total Data Train : ',len(x_train))
print('Total Data Validation : ',len(x_valid))

# merubah data untuk diterima di model
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

# Pemodelan Sequential
train_set = windowed_dataset(x_train, window_size=64, batch_size=200, shuffle_buffer=1000)
val_set = windowed_dataset(x_valid, window_size=64, batch_size=200, shuffle_buffer=1000)
model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.LSTM(60),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
])

# Menghitung nilai 10% MAE
Mae = (df['Total Deaths'].max() - df['Total Deaths'].min()) * 10/100
print(Mae)

# membuat callback dan penggunaan 10% dari skala data
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<6400 and logs.get('val_mae')<2750):
      print("\nMAE dari model < 10% skala data")
      self.model.stop_training = True
callbacks = myCallback()

# menggunakan Learning Rate pada Optimize
optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set, epochs=100, validation_data = val_set, callbacks=[callbacks])

# Plot Accuracy
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Akurasi Model')
plt.ylabel('Mae')
plt.xlabel('epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

# Plot Loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()